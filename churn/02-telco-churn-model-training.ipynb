{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Customer Churn Prediction AI Model\n",
    "Author: Ömer Saatcioglu\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates a machine learning model designed to predict customers at high risk of churn. We utilize a synthetic dataset featuring telco user activities, billing information, and support interactions. The objective is to identify customers likely to change their subscription, as retaining existing customers typically costs less than acquiring new ones, ultimately boosting overall profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "We begin by loading and exploring the synthetic dataset to understand its structure and feature distribution. In practice, we would typically have three separate datasets: CDR (Call Detail Records) for user activities, customer billing information, and support interactions. These datasets would then be integrated into a unified repository for analysis and modeling. For the purpose of this PoC, we’ve consolidated the synthetic data into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install dependencies\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916be9c",
   "metadata": {},
   "source": [
    "This script generates a synthetic telecommunications dataset spanning 36 months for 27,778 unique customers—resulting in approximately 1 million records. It calculates a churn risk score based on usage, billing, and support features, then flags customers as churned if their risk exceeds a defined threshold. The script filters out records following each customer’s first churn event and incorporates a 25% noise factor into the churn risk score to better reflect real-world prediction uncertainties. Finally, the processed dataset is saved to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the customer churn data generator \n",
    "!python3 01-telco-customer-churn-data-generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1817f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the synthetic telecom data\n",
    "data_path = \"data/synthetic_customer_data_evenly_distributed.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data.info()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04752558",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Before training the model, we first preprocess the data by handling missing values, removing date columns, transforming categorical variables into numeric format, and finally splitting the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58847c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values in each column:\", missing_values)\n",
    "\n",
    "# Mapping for PaymentStatus\n",
    "payment_mapping = {\"Paid\": 0, \"Partial\": 0.5, \"Unpaid\": 1}\n",
    "\n",
    "# Mapping for PrimaryIssueType: empty string indicates no support interaction.\n",
    "issue_mapping = {\n",
    "    \"\": 0, \n",
    "    \"Billing\": 0.7, \n",
    "    \"Technical\": 0.8, \n",
    "    \"Service Quality\": 0.6\n",
    "}\n",
    "\n",
    "# Mapping for SupportChannel: adjust the numeric values as needed.\n",
    "support_channel_mapping = {\n",
    "    \"\": 0,       # No support channel if there's no interaction.\n",
    "    \"Phone\": 1,  # Example value: Phone might be considered more direct.\n",
    "    \"Chat\": 0.5, # Example value: Chat might be intermediate.\n",
    "    \"Email\": 0.2 # Example value: Email might be considered less immediate.\n",
    "}\n",
    "\n",
    "# Convert to numeric values\n",
    "data[\"PaymentStatus\"] = data[\"PaymentStatus\"].map(payment_mapping)\n",
    "data[\"PrimaryIssueType\"] = data[\"PrimaryIssueType\"].map(issue_mapping)\n",
    "data[\"SupportChannel\"] = data[\"SupportChannel\"].map(support_channel_mapping)\n",
    "\n",
    "# The date columns\n",
    "date_columns = ['BillingCycleStart', 'BillingCycleEnd', 'PaymentDueDate', 'LastInteractionDate']\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data.drop(columns=['CustomerID', 'HasChurned'] + date_columns, axis=1)\n",
    "y = data['HasChurned']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24442390",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "We will train two models, a Random Forest classifier and a LightGBM classifier, to predict customer churn. After training, we will use these models to generate predictions on our test data and evaluate their performance using several metrics:\n",
    "\n",
    "- Lift by Quantile: Measures the improvement in targeting effectiveness over random selection for each bin of predicted probabilities.\n",
    "- Gini Coefficient: Assesses the model’s discriminatory power by converting the ROC-AUC score into a value that quantifies inequality in prediction performance.\n",
    "- Confusion Matrix: Provides a breakdown of true versus predicted classes, helping to visualize correct and incorrect classifications.\n",
    "- Classification Report: Summarizes key metrics such as precision, recall, and F1-score for each class.\n",
    "- Accuracy Score: Indicates the overall proportion of correctly classified instances.\n",
    "\n",
    "This comprehensive evaluation framework will help us compare the performance of both classifiers and determine which one is better suited for detecting customer churn.\n",
    "\n",
    "### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114126e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and train the BalancedRandomForestClassifier with Fine-tuned hyperparameters \n",
    "model_BRFC = BalancedRandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=200,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    max_depth=100,\n",
    "    bootstrap=True,\n",
    "    sampling_strategy={False: 5000, True: 1000}, \n",
    "    replacement=True  \n",
    ")\n",
    "model_BRFC.fit(X_train, y_train)\n",
    "\n",
    "def compute_lift(y_true, y_scores, n_bins=10):\n",
    "    \"\"\"\n",
    "    Compute lift for each quantile (bin) of predicted probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array-like): True binary labels.\n",
    "        y_scores (array-like): Predicted probabilities for the positive class.\n",
    "        n_bins (int): Number of bins (quantiles) to divide the data into.\n",
    "        \n",
    "    Returns:\n",
    "        lift_df (pd.DataFrame): DataFrame containing the count, positive rate, and lift for each bin.\n",
    "    \"\"\"\n",
    "    # Create DataFrame with true labels and predicted scores\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_scores': y_scores})\n",
    "    # Sort descending by predicted probability\n",
    "    df = df.sort_values('y_scores', ascending=False).reset_index(drop=True)\n",
    "    # Create bins based on the index quantiles\n",
    "    df['bin'] = pd.qcut(df.index, q=n_bins, labels=False)\n",
    "    \n",
    "    # Overall positive rate in the dataset\n",
    "    overall_rate = df['y_true'].mean()\n",
    "    \n",
    "    # Aggregate data by bin: count and average positive rate per bin\n",
    "    lift_df = df.groupby('bin').agg(\n",
    "        count=('y_true', 'count'),\n",
    "        positive_rate=('y_true', 'mean')\n",
    "    ).reset_index()\n",
    "    # Compute lift as the ratio of bin positive rate to overall positive rate\n",
    "    lift_df['lift'] = lift_df['positive_rate'] / overall_rate\n",
    "    \n",
    "    return lift_df\n",
    "\n",
    "def compute_gini(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Compute the Gini coefficient using the ROC-AUC score.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array-like): True binary labels.\n",
    "        y_scores (array-like): Predicted probabilities for the positive class.\n",
    "    \n",
    "    Returns:\n",
    "        gini (float): The Gini coefficient.\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    return 2 * auc - 1\n",
    "\n",
    "y_pred_proba = model_BRFC.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute lift by deciles\n",
    "lift_df = compute_lift(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "# Plot the lift by decile using a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(lift_df['bin'], lift_df['lift'], color='skyblue')\n",
    "plt.xlabel(\"Decile Bin\")\n",
    "plt.ylabel(\"Lift\")\n",
    "plt.title(\"Lift by Decile\")\n",
    "plt.xticks(lift_df['bin'])\n",
    "plt.ylim(0, max(lift_df['lift'])*1.1)  # Extend y-axis slightly for better visual appearance\n",
    "plt.show()\n",
    "\n",
    "# Compute Gini coefficient\n",
    "gini_score = compute_gini(y_test, y_pred_proba)\n",
    "print(f\"\\nGini Score: {gini_score:.4f}\")\n",
    "\n",
    "# Make predictions on the test set with BalancedRandomForestClassifier\n",
    "y_pred = model_BRFC.predict(X_test)\n",
    "# Evaluate the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(\"---------------\")\n",
    "print(\"BalancedRandomForestClassifier Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(acc_score)\n",
    "print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3e9f7",
   "metadata": {},
   "source": [
    "### LightGBM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f98aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Initialize the LightGBM classifier with sample hyperparameters\n",
    "model_LGBM = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=31,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model_LGBM.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = model_LGBM.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute lift by deciles\n",
    "lift_df = compute_lift(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "# Plot the lift by decile using a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(lift_df['bin'], lift_df['lift'], color='skyblue')\n",
    "plt.xlabel(\"Decile Bin\")\n",
    "plt.ylabel(\"Lift\")\n",
    "plt.title(\"Lift by Decile\")\n",
    "plt.xticks(lift_df['bin'])\n",
    "plt.ylim(0, max(lift_df['lift'])*1.1)  # Extend y-axis slightly for better visual appearance\n",
    "plt.show()\n",
    "\n",
    "# Compute Gini coefficient\n",
    "gini_score = compute_gini(y_test, y_pred_proba)\n",
    "print(f\"\\nGini Score: {gini_score:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_LGBM.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"---------------\")\n",
    "print(\"LightGBM Results:\")\n",
    "print(\"Confusion Matrix:\\n\", cm) \n",
    "print(\"Classification Report:\\n\", cr)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"---------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126806d8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we developed a machine learning model to detect churning customers using a synthetic dataset. We employed both a Random Forest classifier and a LightGBM classifier to evaluate our approach.\n",
    "\n",
    "Our Random Forest classifier demonstrated strong performance in identifying potential churners. A key aspect of its configuration was the explicit adjustment of the sampling_strategy, which controls the balance between the majority (non-churn) and minority (churn) classes for each tree. This tailored balancing resulted in a model that, although it slightly reduced recall, significantly increased precision. In practical terms, when the model predicts churn, it is much more likely to be correct, thereby achieving a better trade-off between capturing true churners and minimizing false alarms.\n",
    "\n",
    "In addition, we explored the LightGBM classifier as an alternative. LightGBM builds trees sequentially using a gradient boosting framework, where each new tree is trained to correct the errors (residuals) made by previous trees. This iterative process minimizes a loss function (such as log-loss for classification) through gradient descent, often resulting in faster training times and robust performance, especially on large datasets.\n",
    "\n",
    "Both models reached the accuracy levels designed into our synthetic data, proving the viability of our approach. Looking ahead, applying these models to real-world data will likely require further enhancements through hyperparameter tuning and advanced feature engineering. Moreover, additional evaluation metrics, such as SHAP-based explainability, could provide deeper insights into model performance and its decision-making process.\n",
    "\n",
    "Overall, our findings underscore the potential of both Random Forest and LightGBM classifiers for churn prediction and set the foundation for further refinements and real-world validations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7fad18",
   "metadata": {},
   "source": [
    "## Saving the Models\n",
    "Finally, we will save the trained models to files for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "model_path_BRFC = \"models/customer_churn_prediction_model_brfc.pkl\"\n",
    "model_path_LGBM = \"models/customer_churn_prediction_model_lgbm.pkl\"\n",
    "\n",
    "with open(model_path_BRFC, 'wb') as model_file:\n",
    "    pickle.dump((model_BRFC, X_train.columns.tolist()), model_file)\n",
    "print(f\"Customer Churn Prediction Random Forest classifier Model Saved to {model_path_BRFC}\")\n",
    "\n",
    "with open(model_path_LGBM, 'wb') as model_file:\n",
    "    pickle.dump((model_LGBM, X_train.columns.tolist()), model_file)\n",
    "print(f\"Customer Churn Prediction LightGBM classifier Model Saved to {model_LGBM}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
