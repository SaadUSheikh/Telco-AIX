{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Customer Churn Prediction AI Model\n",
    "Author: Ömer Saatcioglu\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates a machine learning model designed to predict customers at high risk of churn. We utilize a synthetic dataset featuring telco user activities, billing information, and support interactions. The objective is to identify customers likely to change their subscription, as retaining existing customers typically costs less than acquiring new ones, ultimately boosting overall profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "We begin by loading and exploring the synthetic dataset to understand its structure and feature distribution. In practice, we would typically have three separate datasets: CDR (Call Detail Records) for user activities, customer billing information, and support interactions. These datasets would then be integrated into a unified repository for analysis and modeling. For the purpose of this PoC, we’ve consolidated the synthetic data into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install dependencies\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916be9c",
   "metadata": {},
   "source": [
    "This following script generates a synthetic telco dataset spanning 36 months for 27,778 unique customers (around 1 million records). It computes a churn risk score from usage, billing, and support features, flags customers who churn (based on a threshold), filters out customer records after the first churn event, and saves the final dataset to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the customer churn data generator \n",
    "!python3 01-telco-customer-churn-data-generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1817f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the synthetic telecom data\n",
    "data_path = \"data/synthetic_customer_data_evenly_distributed.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data.info()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04752558",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Before training the model, we first preprocess the data by handling missing values, converting dates to Unix timestamps, transforming categorical variables into numeric format, and finally splitting the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58847c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values in each column:\", missing_values)\n",
    "\n",
    "# The date columns that need to be converted\n",
    "date_columns = ['BillingCycleStart', 'BillingCycleEnd', 'PaymentDueDate', 'LastInteractionDate']\n",
    "\n",
    "# Convert each date column to datetime and then to a Unix timestamp as a float\n",
    "for col in date_columns:\n",
    "    data[col] = pd.to_datetime(data[col])\n",
    "    data[col] = data[col].astype('int64') / 1e9\n",
    "\n",
    "# Mapping for PaymentStatus\n",
    "payment_mapping = {\"Paid\": 0, \"Partial\": 0.5, \"Unpaid\": 1}\n",
    "\n",
    "# Mapping for PrimaryIssueType: empty string indicates no support interaction.\n",
    "issue_mapping = {\n",
    "    \"\": 0, \n",
    "    \"Billing\": 0.7, \n",
    "    \"Technical\": 0.8, \n",
    "    \"Service Quality\": 0.6\n",
    "}\n",
    "\n",
    "# Mapping for SupportChannel: adjust the numeric values as needed.\n",
    "support_channel_mapping = {\n",
    "    \"\": 0,       # No support channel if there's no interaction.\n",
    "    \"Phone\": 1,  # Example value: Phone might be considered more direct.\n",
    "    \"Chat\": 0.5, # Example value: Chat might be intermediate.\n",
    "    \"Email\": 0.2 # Example value: Email might be considered less immediate.\n",
    "}\n",
    "\n",
    "# Convert to numeric values\n",
    "data[\"PaymentStatus\"] = data[\"PaymentStatus\"].map(payment_mapping)\n",
    "data[\"PrimaryIssueType\"] = data[\"PrimaryIssueType\"].map(issue_mapping)\n",
    "data[\"SupportChannel\"] = data[\"SupportChannel\"].map(support_channel_mapping)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data.drop(columns=['CustomerID', 'HasChurned'], axis=1)\n",
    "y = data['HasChurned']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24442390",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We will use a Random Forest classifier to train the model. This involves fitting the model on the training data and then making predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114126e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Initialize and train the BalancedRandomForestClassifier with Fine-tuned hyperparameters \n",
    "model = BalancedRandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=200,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    max_depth=100,\n",
    "    bootstrap=True,\n",
    "    sampling_strategy={False: 5000, True: 1000}, \n",
    "    replacement=True  \n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8465a9",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We will evaluate the model's performance using metrics such as confusion matrix, classification report, and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Make predictions on the test set with BalancedRandomForestClassifier\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(\"---------------\")\n",
    "print(\"BalancedRandomForestClassifier Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(acc_score)\n",
    "print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126806d8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we developed a machine learning model to detect churning customers using a synthetic dataset. Our Random Forest classifier performed well in identifying potential churners. By explicitly configuring the sampling_strategy to control the balance between the majority (non-churn) and minority (churn) classes, we achieved a more precise prediction. Although this approach slightly reduced recall, it significantly increased precision, meaning that when the model predicts churn, it is much more likely to be correct. This results in a better trade-off between capturing churners and minimizing false alarms.\n",
    "\n",
    "Further steps could involve hyperparameter tuning, feature engineering, and testing with real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7fad18",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "Finally, we will save the trained model to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "model_path = \"models/customer_churn_prediction_model.pkl\"\n",
    "with open(model_path, 'wb') as model_file:\n",
    "    pickle.dump((model, X_train.columns.tolist()), model_file)\n",
    "print(f\"Customer Churn Prediction BalancedRandomForestClassifier Model Saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
