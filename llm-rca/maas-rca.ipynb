{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model As a Service with RCA after anomaly detection using RAG<br>\n",
    "## Project Overview<br>\n",
    "Author:  Sedat Kaymaz & Fatih E. NAR <br>\n",
    "Update: Adding support of Model as a Service BackEnd. Ref: https://maas.apps.prod.rhoai.rh-aiservices-bu.com/<br> \n",
    "This project aims to provide a root cause analsys method by using LLM with Dynamic RAG based on a system log file <br>\n",
    "for reference after applying anomaly detection ML method to the basic telecom metric list.   <br>\n",
    "Please NOTE: As MaaS does not offer embedding generation service (yet) the correlation between metric anomaly and log datameshing can be poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once only\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your MaaS API key:  4c6eadfbffce00fb01116588c702e790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaaS API key has been set.\n"
     ]
    }
   ],
   "source": [
    "def get_llm():\n",
    "    \"\"\"Retrieve the Language Model (LLM) based on the user's choice of OpenAI or server.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        model_source (str): 'openai' for OpenAI API, 'server' for MaaS Backend.\n",
    "        model_name (str): Model name, defaults to 'gpt-4' for OpenAI.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        ChatOpenAI or dict: LLM object for OpenAI or dictionary with server configuration.\n",
    "\n",
    "    \"\"\"\n",
    "    maas_api_key = os.getenv(\"API_KEY\")\n",
    "    if not maas_api_key:\n",
    "        maas_api_key = input(\"Please enter your MaaS API key: \").strip()\n",
    "        os.environ[\"API_KEY\"] = maas_api_key\n",
    "        print(\"MaaS API key has been set.\")\n",
    "    return {\"server_url\": maas_server_url, \"api_key\": maas_api_key}  # Dictionary format for server configuration\n",
    "\n",
    "MAAS_MAX_CONTEXT_LENGTH = 4000\n",
    "maas_server_url = 'https://mistral-7b-instruct-v0-3-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/chat/completions'\n",
    "llm = get_llm()\n",
    "API_KEY = os.getenv('API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  call_attempt  call_success  call_failure  \\\n",
      "0 2024-09-04 00:00:00           114           110             0   \n",
      "1 2024-09-04 00:01:00           113           110             0   \n",
      "2 2024-09-04 00:02:00           114           111             0   \n",
      "3 2024-09-04 00:03:00           113           111             1   \n",
      "4 2024-09-04 00:04:00           112           111             1   \n",
      "\n",
      "   total_registered_subs  call_success_rate  \n",
      "0                   9031              96.40  \n",
      "1                   9084              97.34  \n",
      "2                   9089              97.36  \n",
      "3                   9035              98.23  \n",
      "4                   9092              99.10  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process metrics file with minimal MaaS embedding update\n",
    "# Only applies to the embedding part to avoid OpenAI dependency for MaaS Backend\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    \"\"\"Custom embedding class for FAISS compatibility, with `embed_documents` and `embed_query` methods.\"\"\"\n",
    "    def embed_documents(self, texts):\n",
    "        # Example embeddings; replace with actual embeddings from the MaaS backend if available\n",
    "        return [np.random.rand(768) for _ in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return np.random.rand(768)  # Example embedding for a single query; replace if available\n",
    "\n",
    "\n",
    "def process_metrics_file(filename):\n",
    "    \"\"\"\n",
    "    Process the metrics file and return a vector store with embeddings based on model source.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the metrics file.\n",
    "        model_source (str): Model source, 'openai' for OpenAI API or 'server' for MaaS Backend.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing the embeddings of the text documents.\n",
    "    \"\"\"\n",
    "    loader = CSVLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = CustomEmbeddings()  # Use CustomEmbeddings for compatibility with FAISS\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "    \n",
    "# Process metrics file\n",
    "metrics_vectorstore = process_metrics_file(\"metrics.csv\")\n",
    "\n",
    "# Load metrics data for anomaly detection\n",
    "df = pd.read_csv(\"data/metrics.csv\")\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies found:\n",
      "                   time  call_attempt  call_success  call_failure  \\\n",
      "683 2024-09-04 11:23:00           114            27             0   \n",
      "684 2024-09-04 11:24:00           113            32             0   \n",
      "685 2024-09-04 11:25:00           112            40             0   \n",
      "686 2024-09-04 11:26:00           114            70             2   \n",
      "\n",
      "     total_registered_subs  call_success_rate  is_anomaly  \n",
      "683                   9029             23.491          -1  \n",
      "684                   9038             28.368          -1  \n",
      "685                   9033             35.730          -1  \n",
      "686                   9157             61.491          -1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anomaly detection using Isolation Forest\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"\n",
    "    Detects anomalies in the given DataFrame using the Isolation Forest algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing the data to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A subset of the input DataFrame containing only the rows that are classified as anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    features = ['call_attempt', 'call_success', 'call_failure', 'total_registered_subs', 'call_success_rate']\n",
    "    X = df[features]\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.005, random_state=42)\n",
    "    anomalies = iso_forest.fit_predict(X)\n",
    "    \n",
    "    df['is_anomaly'] = anomalies\n",
    "    return df[df['is_anomaly'] == -1]\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = detect_anomalies(df)\n",
    "\n",
    "if anomalies.empty:\n",
    "    print(\"No anomalies detected in the metrics.\")\n",
    "\n",
    "print(f\"Anomalies found:\\n{anomalies}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing log file...\n",
      "Log file has been processed.\n"
     ]
    }
   ],
   "source": [
    "# RAG for processing log file\n",
    "def process_log_file(filename):\n",
    "    \"\"\"\n",
    "    Process a log file and return a vector store.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the log file to process.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing embeddings of the log file texts.\n",
    "    \"\"\"\n",
    "    loader = TextLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = CustomEmbeddings()  # Use CustomEmbeddings for compatibility with FAISS\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Process log file\n",
    "print(\"Processing log file...\")\n",
    "logs_vectorstore = process_log_file(\"systemd.log\")\n",
    "print(\"Log file has been processed.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Cause Analysis:\n",
      " To perform a root cause analysis on the provided anomalies, metrics, and logs, I will first analyze the anomalies and then correlate them with the logs and metrics.\n",
      "\n",
      "Anomaly Analysis:\n",
      "The anomalies provided show inconsistent call_attempt, call_success, and call_success_rate. Here's a summary of the anomalies:\n",
      "\n",
      "- The call_attempt is consistently decreasing from 114 to 112, then suddenly increases to 114, and finally increases to 114 again.\n",
      "- The call_success is 112 for three consecutive data points, then suddenly drops to 110, and remains constant.\n",
      "- The call_success_rate fluctuates randomly, ranging from 97.34% to 100.0%.\n",
      "\n",
      "Given the inconsistency in the call-related metrics, it's reasonable to suspect that there might be an issue with the call management system or the network.\n",
      "\n",
      "Correlating Anomalies with Logs:\n",
      "The logs provided only show the Startup and Initialization messages of the OpenStack Compute Service (nova-compute) and do not contain any error or warning messages related to call management or network issues. However, it's worth considering whether any changes or updates to the OpenStack Compute Service could have caused these anomalies.\n",
      "\n",
      "Correlating Anomalies with Metrics:\n",
      "The provided metrics show the state of the system at specific points in time. However, they do not provide enough information to directly correlate with the anomalies. It would be helpful to have more metrics, such as network-related metrics, during the anomalous time frames to identify potential issues.\n",
      "\n",
      "Actionable Steps:\n",
      "1. Investigate the call management system for any known issues that might cause these anomalies.\n",
      "2. Review the system logs for more detailed information about the OpenStack Compute Service and its interactions with the call management system.\n",
      "3. Collect network-related metrics during the anomalous time frames to identify potential network issues.\n",
      "4. Perform stress tests on the call management system and OpenStack Compute Service to identify any performance bottlenecks or stability issues that might cause these anomalies.\n",
      "5. If necessary, consult with the system administrators or vendors for further assistance.\n",
      "\n",
      "By following these steps, we can determine the root cause of the anomalies and take appropriate actions to resolve the issue.\n"
     ]
    }
   ],
   "source": [
    "def analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies):\n",
    "    # Prepare the anomalies list\n",
    "    anomalies_list = anomalies.astype(str).values.tolist() if isinstance(anomalies, pd.DataFrame) else anomalies\n",
    "\n",
    "    # Retrieve related metrics and logs information\n",
    "    metrics_embedding = metrics_vectorstore.embedding_function.embed_query(str(anomalies_list))\n",
    "    logs_embedding = logs_vectorstore.embedding_function.embed_query(str(anomalies_list))\n",
    "\n",
    "    metrics_info = [doc.page_content[:500] for doc in metrics_vectorstore.similarity_search_by_vector(metrics_embedding, k=2)]  # Truncate to limit token usage\n",
    "    logs_info = [doc.page_content[:500] for doc in logs_vectorstore.similarity_search_by_vector(logs_embedding, k=2)]  # Truncate to limit token usage\n",
    "\n",
    "    # Prepare headers and payload for the MaaS server\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': API_KEY,\n",
    "    }\n",
    "\n",
    "    # Prepare content with token count calculation\n",
    "    messages_content = f\"Anomalies: {anomalies_list}\\nMetrics Info: {metrics_info}\\nLogs Info: {logs_info}\"\n",
    "    message_token_count = len(messages_content.split())  # Rough token approximation based on word count\n",
    "    max_tokens = max(0, 6144 - message_token_count - 550)  # Buffer for completion tokens\n",
    "\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": \"You are a root cause analysis assistant. Provide a detailed analysis of anomalies using the provided metrics and logs.\",\n",
    "                \"role\": \"system\",\n",
    "                \"name\": \"system\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": messages_content,\n",
    "                \"role\": \"user\",\n",
    "                \"name\": \"user\"\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"mistral-7b-instruct\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"response_format\": {\n",
    "            \"type\": \"text\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Send POST request to the MaaS server\n",
    "    response = requests.post(maas_server_url, headers=headers, json=data)\n",
    "\n",
    "    # Process the MaaS response\n",
    "    try:\n",
    "        response_data = response.json()\n",
    "        if 'choices' in response_data:\n",
    "            result = response_data['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(\"Unexpected response structure:\", response_data)\n",
    "            result = \"Unexpected response format received from server. Check server logs for details.\"\n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        print(\"Failed to parse the response from the server:\", response.text)\n",
    "        result = \"Failed to parse the response from the server. Check server logs for more information.\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# Perform root cause analysis\n",
    "analysis = analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies)\n",
    "print(\"Root Cause Analysis:\")\n",
    "print(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
