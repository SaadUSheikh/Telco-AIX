{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM with RCA after anomaly detection using RAG<br>\n",
    "## Project Overview<br>\n",
    "Author: Sedat Kaymaz<br>\n",
    "This project aims to provide a root cause analsys method by using LLM with Dynamic RAG based on a system log file <br>\n",
    "for reference after applying anomaly detection ML method to the basic telecom metric list   <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once only\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language Model...\n",
      "Language Model gpt-4 loaded.\n"
     ]
    }
   ],
   "source": [
    "# LLM Configuration\n",
    "def get_llm(model):\n",
    "    \"\"\"\n",
    "    Retrieves the Language Model (LLM) for root cause analysis.\n",
    "\n",
    "    This function retrieves the Language Model (LLM) used for root cause analysis. It first checks if the OpenAI API key is set as an environment variable. If not, it prompts the user to enter the API key and sets it as an environment variable. The function then returns an instance of the ChatOpenAI class with a temperature of 0 and the model name set to \"gpt-4\".\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: An instance of the ChatOpenAI class representing the Language Model (LLM) for root cause analysis.\n",
    "    \"\"\"\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_api_key:\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    \n",
    "    # You can modify this to use different models or local LLMs\n",
    "    return ChatOpenAI(temperature=0, model_name=model)\n",
    "\n",
    "print(\"Loading Language Model...\")\n",
    "model=\"gpt-4\"\n",
    "llm = get_llm(model)\n",
    "print(f\"Language Model {model} loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  call_attempt  call_success  call_failure  \\\n",
      "0 2024-09-04 00:00:00           114           110             0   \n",
      "1 2024-09-04 00:01:00           113           110             0   \n",
      "2 2024-09-04 00:02:00           114           111             0   \n",
      "3 2024-09-04 00:03:00           113           111             1   \n",
      "4 2024-09-04 00:04:00           112           111             1   \n",
      "\n",
      "   total_registered_subs  call_success_rate  \n",
      "0                   9031              96.40  \n",
      "1                   9084              97.34  \n",
      "2                   9089              97.36  \n",
      "3                   9035              98.23  \n",
      "4                   9092              99.10  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RAG for reading and processing metrics file\n",
    "def process_metrics_file(filename):\n",
    "    \"\"\"\n",
    "    Process the metrics file and return a vector store.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the metrics file.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing the embeddings of the text documents.\n",
    "    \"\"\"\n",
    "    loader = CSVLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Process metrics file\n",
    "metrics_vectorstore = process_metrics_file(\"metrics.csv\")\n",
    "\n",
    "# Load metrics data for anomaly detection\n",
    "df = pd.read_csv(\"data/metrics.csv\")\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies found:\n",
      "                   time  call_attempt  call_success  call_failure  \\\n",
      "683 2024-09-04 11:23:00           114            27             0   \n",
      "684 2024-09-04 11:24:00           113            32             0   \n",
      "685 2024-09-04 11:25:00           112            40             0   \n",
      "686 2024-09-04 11:26:00           114            70             2   \n",
      "\n",
      "     total_registered_subs  call_success_rate  is_anomaly  \n",
      "683                   9029             23.491          -1  \n",
      "684                   9038             28.368          -1  \n",
      "685                   9033             35.730          -1  \n",
      "686                   9157             61.491          -1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anomaly detection using Isolation Forest\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"\n",
    "    Detects anomalies in the given DataFrame using the Isolation Forest algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing the data to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A subset of the input DataFrame containing only the rows that are classified as anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    features = ['call_attempt', 'call_success', 'call_failure', 'total_registered_subs', 'call_success_rate']\n",
    "    X = df[features]\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.005, random_state=42)\n",
    "    anomalies = iso_forest.fit_predict(X)\n",
    "    \n",
    "    df['is_anomaly'] = anomalies\n",
    "    return df[df['is_anomaly'] == -1]\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = detect_anomalies(df)\n",
    "if anomalies.empty:\n",
    "    print(\"No anomalies detected in the metrics.\")\n",
    "\n",
    "print(f\"Anomalies found:\\n{anomalies}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing log file...\n",
      "Log file has been processed.\n"
     ]
    }
   ],
   "source": [
    "# RAG for processing log file\n",
    "def process_log_file(filename):\n",
    "    \"\"\"\n",
    "    Process a log file and return a vector store.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the log file to process.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing embeddings of the log file texts.\n",
    "    \"\"\"\n",
    "    loader = TextLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# Process log file\n",
    "print(\"Processing log file...\")\n",
    "logs_vectorstore = process_log_file(\"systemd.log\")\n",
    "print(\"Log file has been processed.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Cause Analysis:\n",
      "Based on the provided logs and metrics, the anomalies in the metrics seem to be related to the OpenStack services, specifically the Open vSwitch service and the Nova Compute service.\n",
      "\n",
      "At 11:22:13, there is a log entry indicating an assertion failure in the Open vSwitch service, which leads to the service being killed and restarted. This could potentially disrupt network connectivity for the OpenStack services, affecting call attempts and successes.\n",
      "\n",
      "The Nova Compute service logs show several instances being migrated, rebooted, created, and shut down around the same time. This could potentially cause disruptions in the service, affecting the call success rate. Specifically, at 11:23:01, there is a log entry indicating the start of a migration for an instance, which could potentially disrupt the service.\n",
      "\n",
      "In addition, the total number of registered subscribers increases from 9033 to 9157 between 11:25:00 and 11:26:00. This sudden increase could potentially overload the system, leading to a decrease in the call success rate.\n",
      "\n",
      "In conclusion, the anomalies in the metrics could be caused by disruptions in the OpenStack services due to the Open vSwitch service failure and the Nova Compute service operations, as well as a sudden increase in the number of registered subscribers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Root cause analysis using LLM\n",
    "def analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies):\n",
    "    \"\"\"\n",
    "    Analyzes the root cause of anomalies in the metrics based on the system logs.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM): The LLM object used for analysis.\n",
    "        metrics_vectorstore (VectorStore): The vector store containing metrics information.\n",
    "        logs_vectorstore (VectorStore): The vector store containing log information.\n",
    "        anomalies (list): A list of anomalies to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        str: A detailed root cause analysis for the given anomalies.\n",
    "\n",
    "    Example:\n",
    "        anomalies = [\"Anomaly1\", \"Anomaly2\"]\n",
    "        result = analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies)\n",
    "        print(result)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    Analyze the following anomalies in the metrics and provide a root cause analysis based on the system logs:\n",
    "\n",
    "    Anomalies:\n",
    "    {anomalies}\n",
    "\n",
    "    Relevant metrics information:\n",
    "    {metrics_info}\n",
    "\n",
    "    Relevant log information:\n",
    "    {logs_info}\n",
    "\n",
    "    Please provide a detailed root cause analysis for these anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"anomalies\", \"metrics_info\", \"logs_info\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    # Create a RunnableSequence\n",
    "    chain = (\n",
    "        {\n",
    "            \"anomalies\": RunnablePassthrough(),\n",
    "            \"metrics_info\": lambda x: metrics_vectorstore.similarity_search(str(x), k=2),\n",
    "            \"logs_info\": lambda x: logs_vectorstore.similarity_search(str(x), k=2)\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Invoke the chain\n",
    "    result = chain.invoke(anomalies)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Perform root cause analysis\n",
    "analysis = analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies)\n",
    "\n",
    "print(\"Root Cause Analysis:\")\n",
    "print(analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
