{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b161ea",
   "metadata": {},
   "source": [
    "# LLM with RCA after Anomaly Detection Using RAG\n",
    "## Project Overview\n",
    "This project aims to perform root cause analysis (RCA) using a large language model (LLM) with dynamic Retrieve-and-Generate (RAG) based on AMF, SMF, and UPF telecom metrics. The workflow includes anomaly detection and correlation analysis across the datasets, leveraging system logs for RCA insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc731b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies (if not already installed)\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af538925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser, Document \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adfd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the LLM\n",
    "def get_llm(model='gpt-4'):\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    return ChatOpenAI(temperature=0, model_name=model)\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6adfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess telecom metrics (AMF, SMF, UPF)\n",
    "def load_metrics(filenames):\n",
    "    datasets = {}\n",
    "    for name, file in filenames.items():\n",
    "        df = pd.read_csv(file)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df.sort_values('timestamp', inplace=True)\n",
    "        datasets[name] = df\n",
    "    return datasets\n",
    "\n",
    "metric_files = {\n",
    "    'amf': 'data/amf_metrics.csv',\n",
    "    'smf': 'data/smf_metrics.csv',\n",
    "    'upf': 'data/upf_metrics.csv'\n",
    "}\n",
    "\n",
    "metrics_data = load_metrics(metric_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071b9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   timestamp  http_connectivity  cpu_utilization  \\\n",
      "0 2025-01-16 12:01:55.775630              False            False   \n",
      "1 2025-01-16 12:02:55.775630              False            False   \n",
      "2 2025-01-16 12:03:55.775630              False            False   \n",
      "3 2025-01-16 12:04:55.775630              False            False   \n",
      "4 2025-01-16 12:05:55.775630              False            False   \n",
      "\n",
      "   memory_utilization  registration_rate  session_setup_rate  \\\n",
      "0               False              False               False   \n",
      "1               False              False               False   \n",
      "2               False              False               False   \n",
      "3               False              False               False   \n",
      "4               False              False               False   \n",
      "\n",
      "   authentication_success_rate  n1n2_message_rate  registration_success_rate  \\\n",
      "0                        False              False                      False   \n",
      "1                        False              False                      False   \n",
      "2                        False              False                      False   \n",
      "3                        False              False                      False   \n",
      "4                        False              False                      False   \n",
      "\n",
      "   slice_selection_success_rate  nas_security_success_rate  ngap_success_rate  \n",
      "0                         False                      False              False  \n",
      "1                         False                      False              False  \n",
      "2                         False                      False              False  \n",
      "3                         False                      False              False  \n",
      "4                         False                      False              False  \n",
      "                   timestamp  cpu_utilization  memory_utilization  \\\n",
      "0 2025-01-16 12:01:55.775630            False               False   \n",
      "1 2025-01-16 12:02:55.775630            False               False   \n",
      "2 2025-01-16 12:03:55.775630            False               False   \n",
      "3 2025-01-16 12:04:55.775630            False               False   \n",
      "4 2025-01-16 12:05:55.775630            False               False   \n",
      "\n",
      "   session_establishment_rate  n4_message_rate  session_modification_rate  \\\n",
      "0                       False            False                      False   \n",
      "1                       False            False                      False   \n",
      "2                       False            False                      False   \n",
      "3                       False            False                      False   \n",
      "4                       False            False                      False   \n",
      "\n",
      "   policy_installation_rate  pfcp_message_rate  session_success_rate  \n",
      "0                     False              False                 False  \n",
      "1                     False              False                 False  \n",
      "2                     False              False                 False  \n",
      "3                     False              False                 False  \n",
      "4                     False              False                 False  \n",
      "                   timestamp  cpu_utilization  memory_utilization  \\\n",
      "0 2025-01-16 12:01:55.775630            False               False   \n",
      "1 2025-01-16 12:02:55.775630            False               False   \n",
      "2 2025-01-16 12:03:55.775630            False               False   \n",
      "3 2025-01-16 12:04:55.775630            False               False   \n",
      "4 2025-01-16 12:05:55.775630            False               False   \n",
      "\n",
      "   packet_processing_rate  tunnel_establishment_rate  buffer_utilization  \\\n",
      "0                   False                      False               False   \n",
      "1                   False                      False               False   \n",
      "2                   False                      False               False   \n",
      "3                   False                      False               False   \n",
      "4                   False                      False               False   \n",
      "\n",
      "   qos_flow_success_rate  throughput_mbps  packet_drop_rate  latency_ms  \n",
      "0                  False            False             False       False  \n",
      "1                  False            False             False       False  \n",
      "2                  False            False             False       False  \n",
      "3                  False            False             False       False  \n",
      "4                  False            False             False       False  \n"
     ]
    }
   ],
   "source": [
    "# Detect anomalies using z-score\n",
    "def detect_anomalies(datasets, threshold=3):\n",
    "    anomalies = {}\n",
    "    for name, df in datasets.items():\n",
    "        numeric_data = df.drop(columns=['timestamp'])\n",
    "        anomaly_flags = (np.abs((numeric_data - numeric_data.mean()) / numeric_data.std()) > threshold)\n",
    "        anomalies[name] = pd.concat([df[['timestamp']], anomaly_flags], axis=1)\n",
    "    return anomalies\n",
    "\n",
    "anomalies = detect_anomalies(metrics_data)\n",
    "print(anomalies['amf'].head())\n",
    "print(anomalies['smf'].head())  \n",
    "print(anomalies['upf'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c7d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a description column to each dataset\n",
    "def add_description_column(datasets):\n",
    "    \"\"\"\n",
    "    Add a description column to each dataset for embedding purposes.\n",
    "\n",
    "    Args:\n",
    "        datasets (dict): A dictionary of dataset names and DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated dictionary with description columns added.\n",
    "    \"\"\"\n",
    "    for name, df in datasets.items():\n",
    "        df['description'] = df.apply(\n",
    "            lambda row: f\"{row['timestamp']} - \" +\n",
    "                        \" | \".join([f\"{col}: {row[col]}\" for col in df.columns if col != 'timestamp']),\n",
    "            axis=1\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "# Add descriptions to datasets\n",
    "metrics_data = add_description_column(metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20cc6c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     timestamp  http_connectivity  cpu_utilization_smf_amf  \\\n",
      "885 2025-01-17 02:46:55.775630              False                    False   \n",
      "\n",
      "     memory_utilization_smf_amf  registration_rate  session_setup_rate  \\\n",
      "885                       False              False               False   \n",
      "\n",
      "     authentication_success_rate  n1n2_message_rate  \\\n",
      "885                        False              False   \n",
      "\n",
      "     registration_success_rate  slice_selection_success_rate  ...  \\\n",
      "885                      False                         False  ...   \n",
      "\n",
      "     cpu_utilization  memory_utilization  packet_processing_rate  \\\n",
      "885            False               False                   False   \n",
      "\n",
      "     tunnel_establishment_rate  buffer_utilization  qos_flow_success_rate  \\\n",
      "885                      False               False                  False   \n",
      "\n",
      "     throughput_mbps  packet_drop_rate  latency_ms  combined_anomaly  \n",
      "885            False             False        True              True  \n",
      "\n",
      "[1 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge anomalies for correlation analysis\n",
    "def correlate_anomalies(anomalies):\n",
    "    combined = anomalies['amf']\n",
    "    for name, df in anomalies.items():\n",
    "        if name != 'amf':\n",
    "            combined = pd.merge(combined, df, on='timestamp', suffixes=(f'_{name}_amf', f'_{name}'))\n",
    "    combined['combined_anomaly'] = combined.drop(columns=['timestamp']).any(axis=1)\n",
    "    return combined[combined['combined_anomaly']]\n",
    "\n",
    "correlated_anomalies = correlate_anomalies(anomalies)\n",
    "print(correlated_anomalies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa11c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process metrics for RAG\n",
    "def process_metrics(datasets, column_to_embed):\n",
    "    \"\"\"\n",
    "    Process multiple metrics datasets to create individual vector stores for RAG.\n",
    "\n",
    "    Args:\n",
    "        datasets (dict): A dictionary of dataset names and DataFrames.\n",
    "        column_to_embed (str): The column containing text or descriptions to embed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of vector stores for each dataset.\n",
    "    \"\"\"\n",
    "    vector_stores = {}\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        if column_to_embed not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_to_embed}' not found in the {name} DataFrame.\")\n",
    "\n",
    "        # Convert the column to a list of Document objects\n",
    "        documents = [Document(page_content=row[column_to_embed]) for _, row in df.iterrows()]\n",
    "\n",
    "        # Split the documents into chunks (if applicable)\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Embed the chunks\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        vector_stores[name] = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    return vector_stores\n",
    "\n",
    "# Example usage\n",
    "# Assume `metrics_data` is a dictionary containing the AMF, SMF, and UPF DataFrames.\n",
    "# Each DataFrame should have a column 'description' with text to embed.\n",
    "vector_stores = process_metrics(metrics_data, column_to_embed='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05daebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alerts from JSON\n",
    "def load_alerts(file_path):\n",
    "    \"\"\"\n",
    "    Load alerts from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed alert data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load alerts from the `data/` folder\n",
    "alerts = load_alerts('data/alerts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc4602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided logs, the anomaly detected is related to the latency_ms metric, which is a measure of the delay in data transmission. This metric is flagged as True in the anomaly column, indicating that there is an issue with the latency in the system.\n",
      "\n",
      "Looking at the relevant UPF metrics information, we can see that the latency_ms value has increased from 13.37484865015994 at 09:46:55.775630 to 13.86797014781209 at 11:17:55.775630. This increase in latency could be due to several reasons such as network congestion, hardware issues, or software issues.\n",
      "\n",
      "The CPU utilization and memory utilization have also increased during this time period. The CPU utilization has increased from 45.181238864488 to 50.542563686731 and the memory utilization has increased from 58.74739464810597 to 65.30386944707406. This increase in resource utilization could be contributing to the increased latency as the system may be struggling to process data efficiently due to the high resource usage.\n",
      "\n",
      "The packet_processing_rate has decreased from 13404.863581966032 to 12357.272237015131, which could also be contributing to the increased latency. If the system is processing packets at a slower rate, this could lead to delays in data transmission.\n",
      "\n",
      "The buffer_utilization has also increased from 37.98646570464442 to 40.893310839631, indicating that the system is storing more data in the buffer. This could be a result of the system struggling to process data efficiently, leading to an increase in latency.\n",
      "\n",
      "In conclusion, the root cause of the anomaly in the latency_ms metric could be due to high CPU and memory utilization, a decrease in packet processing rate, and an increase in buffer utilization. Further investigation would be needed to determine the exact cause and to implement appropriate solutions.\n"
     ]
    }
   ],
   "source": [
    "# Analyze anomalies using LLM\n",
    "def analyze_root_cause(llm, vector_stores, anomalies):\n",
    "    \"\"\"\n",
    "    Analyze the root cause of anomalies using LLM and vector stores.\n",
    "\n",
    "    Args:\n",
    "        llm (ChatOpenAI): The LLM object for analysis.\n",
    "        vector_stores (dict): A dictionary of vector stores for AMF, SMF, and UPF.\n",
    "        anomalies (pd.DataFrame): The DataFrame containing correlated anomalies.\n",
    "\n",
    "    Returns:\n",
    "        str: The root cause analysis generated by the LLM.\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Analyze the following anomalies in the metrics and provide a root cause analysis based on the system logs:\n",
    "\n",
    "    Anomalies:\n",
    "    {anomalies}\n",
    "\n",
    "    Relevant AMF metrics information:\n",
    "    {amf_metrics}\n",
    "\n",
    "    Relevant SMF metrics information:\n",
    "    {smf_metrics}\n",
    "\n",
    "    Relevant UPF metrics information:\n",
    "    {upf_metrics}\n",
    "\n",
    "    Provide a detailed root cause analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"anomalies\", \"amf_metrics\", \"smf_metrics\", \"upf_metrics\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    # Function to fetch relevant metrics information from vector stores\n",
    "    def fetch_relevant_metrics(vector_store, anomalies):\n",
    "        results = []\n",
    "        for anomaly in anomalies.to_dict(orient=\"records\"):\n",
    "            results.extend(vector_store.similarity_search(str(anomaly), k=2))\n",
    "        return \"\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    # Fetch relevant metrics information\n",
    "    amf_metrics = fetch_relevant_metrics(vector_stores['amf'], anomalies)\n",
    "    smf_metrics = fetch_relevant_metrics(vector_stores['smf'], anomalies)\n",
    "    upf_metrics = fetch_relevant_metrics(vector_stores['upf'], anomalies)\n",
    "\n",
    "    # Combine data into the chain\n",
    "    chain = ({\n",
    "        \"anomalies\": RunnablePassthrough(),\n",
    "        \"amf_metrics\": lambda x: amf_metrics,\n",
    "        \"smf_metrics\": lambda x: smf_metrics,\n",
    "        \"upf_metrics\": lambda x: upf_metrics\n",
    "    } | prompt | llm | StrOutputParser())\n",
    "\n",
    "    return chain.invoke(str(anomalies))\n",
    "\n",
    "# Example usage\n",
    "root_cause_analysis = analyze_root_cause(llm, vector_stores, correlated_anomalies)\n",
    "print(root_cause_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af532d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided system logs and alerts, the anomalies in the metrics can be attributed to several issues affecting different components of the system. \n",
      "\n",
      "1. Registration Storm: Both the Access and Mobility Management Function (AMF) and Session Management Function (SMF) were affected by a registration storm. This is a situation where a large number of devices attempt to register with the network simultaneously, causing a surge in registration rate. This can lead to increased CPU and memory utilization as the system tries to process the high volume of registration requests. \n",
      "\n",
      "2. Resource Exhaustion: Both the User Plane Function (UPF) and SMF experienced resource exhaustion. This indicates that these components were running out of resources, likely due to high demand. This can be seen in the increased CPU and memory utilization metrics for these components. \n",
      "\n",
      "3. Session Management Failure: The AMF, SMF, and UPF all experienced session management failures. This suggests that there were issues with establishing, maintaining, or terminating sessions. This could be due to a variety of reasons, including network congestion, software bugs, or hardware failures. \n",
      "\n",
      "4. Latency: The UPF experienced increased latency, as indicated by the 'True' value in the 'latency_ms' column of the anomalies table. This could be a result of the aforementioned issues, as increased CPU and memory utilization, as well as session management failures, can all contribute to increased latency. \n",
      "\n",
      "In conclusion, the root cause of the anomalies appears to be a combination of a registration storm, resource exhaustion, and session management failures, all of which led to increased CPU and memory utilization and increased latency. Further investigation would be needed to determine the exact cause of these issues and to develop appropriate solutions.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RCA with alerts\n",
    "def analyze_with_alerts(llm, vector_stores, anomalies, alerts):\n",
    "    \"\"\"\n",
    "    Enhance RCA with alerts information.\n",
    "\n",
    "    Args:\n",
    "        llm (ChatOpenAI): The LLM for analysis.\n",
    "        vector_stores (dict): Vector stores for AMF, SMF, and UPF metrics.\n",
    "        anomalies (pd.DataFrame): DataFrame containing anomalies.\n",
    "        alerts (dict): Parsed alert data from alerts.json.\n",
    "\n",
    "    Returns:\n",
    "        str: Detailed RCA combining anomalies and alert information.\n",
    "    \"\"\"\n",
    "    # Parse alerts\n",
    "    alert_details = \"\\n\".join([\n",
    "        f\"Type: {alert['type']}, Severity: {alert['severity']}, Component: {alert['component']}, \"\n",
    "        f\"Start: {alert['start_time']}, End: {alert['end_time']}, Description: {alert['description']}\"\n",
    "        for alert in alerts['alerts']\n",
    "    ])\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt_template = \"\"\"\n",
    "    Analyze the following anomalies in the metrics and provide a root cause analysis based on the system logs and alerts:\n",
    "\n",
    "    Anomalies:\n",
    "    {anomalies}\n",
    "\n",
    "    Alerts:\n",
    "    {alerts}\n",
    "\n",
    "    Relevant AMF metrics information:\n",
    "    {amf_metrics}\n",
    "\n",
    "    Relevant SMF metrics information:\n",
    "    {smf_metrics}\n",
    "\n",
    "    Relevant UPF metrics information:\n",
    "    {upf_metrics}\n",
    "\n",
    "    Provide a detailed root cause analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"anomalies\", \"alerts\", \"amf_metrics\", \"smf_metrics\", \"upf_metrics\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    # Fetch relevant metrics\n",
    "    def fetch_metrics(vector_store, anomalies):\n",
    "        results = []\n",
    "        for anomaly in anomalies.to_dict(orient=\"records\"):\n",
    "            results.extend(vector_store.similarity_search(str(anomaly), k=2))\n",
    "        return \"\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    amf_metrics = fetch_metrics(vector_stores['amf'], anomalies)\n",
    "    smf_metrics = fetch_metrics(vector_stores['smf'], anomalies)\n",
    "    upf_metrics = fetch_metrics(vector_stores['upf'], anomalies)\n",
    "\n",
    "    # Combine into chain\n",
    "    chain = ({\n",
    "        \"anomalies\": RunnablePassthrough(),\n",
    "        \"alerts\": lambda x: alert_details,\n",
    "        \"amf_metrics\": lambda x: amf_metrics,\n",
    "        \"smf_metrics\": lambda x: smf_metrics,\n",
    "        \"upf_metrics\": lambda x: upf_metrics\n",
    "    } | prompt | llm | StrOutputParser())\n",
    "\n",
    "    return chain.invoke(str(anomalies))\n",
    "\n",
    "# Example Usage\n",
    "rca_with_alerts = analyze_with_alerts(llm, vector_stores, correlated_anomalies, alerts)\n",
    "print(rca_with_alerts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
