{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b161ea",
   "metadata": {},
   "source": [
    "# LLM with RCA after Anomaly Detection Using RAG\n",
    "## Project Overview\n",
    "This project aims to perform root cause analysis (RCA) using a large language model (LLM) with dynamic Retrieve-and-Generate (RAG) based on AMF, SMF, and UPF telecom metrics. The workflow includes anomaly detection and correlation analysis across the datasets, leveraging system logs for RCA insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc731b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies (if not already installed)\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af538925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser, Document \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adfd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the LLM\n",
    "def get_llm(model='gpt-4'):\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    return ChatOpenAI(temperature=0, model_name=model)\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6adfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess telecom metrics (AMF, SMF, UPF)\n",
    "def load_metrics(filenames):\n",
    "    datasets = {}\n",
    "    for name, file in filenames.items():\n",
    "        df = pd.read_csv(file)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df.sort_values('timestamp', inplace=True)\n",
    "        datasets[name] = df\n",
    "    return datasets\n",
    "\n",
    "metric_files = {\n",
    "    'amf': 'data/amf_metrics.csv',\n",
    "    'smf': 'data/smf_metrics.csv',\n",
    "    'upf': 'data/upf_metrics.csv'\n",
    "}\n",
    "\n",
    "metrics_data = load_metrics(metric_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071b9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   timestamp  http_connectivity  cpu_utilization  \\\n",
      "0 2025-01-13 10:23:31.133476              False            False   \n",
      "1 2025-01-13 10:24:31.133476              False            False   \n",
      "2 2025-01-13 10:25:31.133476              False            False   \n",
      "3 2025-01-13 10:26:31.133476              False            False   \n",
      "4 2025-01-13 10:27:31.133476              False            False   \n",
      "\n",
      "   memory_utilization  registration_rate  session_setup_rate  \\\n",
      "0               False              False               False   \n",
      "1               False              False               False   \n",
      "2               False              False               False   \n",
      "3               False              False               False   \n",
      "4               False              False               False   \n",
      "\n",
      "   authentication_success_rate  n1n2_message_rate  registration_success_rate  \\\n",
      "0                        False              False                      False   \n",
      "1                        False              False                      False   \n",
      "2                        False              False                      False   \n",
      "3                        False              False                      False   \n",
      "4                        False              False                      False   \n",
      "\n",
      "   slice_selection_success_rate  nas_security_success_rate  ngap_success_rate  \n",
      "0                         False                      False              False  \n",
      "1                         False                      False              False  \n",
      "2                         False                      False              False  \n",
      "3                         False                      False              False  \n",
      "4                         False                      False              False  \n",
      "                   timestamp  cpu_utilization  memory_utilization  \\\n",
      "0 2025-01-13 10:23:31.133476            False               False   \n",
      "1 2025-01-13 10:24:31.133476            False               False   \n",
      "2 2025-01-13 10:25:31.133476            False               False   \n",
      "3 2025-01-13 10:26:31.133476            False               False   \n",
      "4 2025-01-13 10:27:31.133476            False               False   \n",
      "\n",
      "   session_establishment_rate  n4_message_rate  session_modification_rate  \\\n",
      "0                       False            False                      False   \n",
      "1                       False            False                      False   \n",
      "2                       False            False                      False   \n",
      "3                       False            False                      False   \n",
      "4                       False            False                      False   \n",
      "\n",
      "   policy_installation_rate  pfcp_message_rate  session_success_rate  \n",
      "0                     False              False                 False  \n",
      "1                     False              False                 False  \n",
      "2                     False              False                 False  \n",
      "3                     False              False                 False  \n",
      "4                     False              False                 False  \n",
      "                   timestamp  cpu_utilization  memory_utilization  \\\n",
      "0 2025-01-13 10:23:31.133476            False               False   \n",
      "1 2025-01-13 10:24:31.133476            False               False   \n",
      "2 2025-01-13 10:25:31.133476            False               False   \n",
      "3 2025-01-13 10:26:31.133476            False               False   \n",
      "4 2025-01-13 10:27:31.133476            False               False   \n",
      "\n",
      "   packet_processing_rate  tunnel_establishment_rate  buffer_utilization  \\\n",
      "0                   False                      False               False   \n",
      "1                   False                      False               False   \n",
      "2                   False                      False               False   \n",
      "3                   False                      False               False   \n",
      "4                   False                      False               False   \n",
      "\n",
      "   qos_flow_success_rate  throughput_mbps  packet_drop_rate  latency_ms  \n",
      "0                  False            False             False       False  \n",
      "1                  False            False             False       False  \n",
      "2                  False            False             False       False  \n",
      "3                  False            False             False       False  \n",
      "4                  False            False             False       False  \n"
     ]
    }
   ],
   "source": [
    "# Detect anomalies using z-score\n",
    "def detect_anomalies(datasets, threshold=3):\n",
    "    anomalies = {}\n",
    "    for name, df in datasets.items():\n",
    "        numeric_data = df.drop(columns=['timestamp'])\n",
    "        anomaly_flags = (np.abs((numeric_data - numeric_data.mean()) / numeric_data.std()) > threshold)\n",
    "        anomalies[name] = pd.concat([df[['timestamp']], anomaly_flags], axis=1)\n",
    "    return anomalies\n",
    "\n",
    "anomalies = detect_anomalies(metrics_data)\n",
    "print(anomalies['amf'].head())\n",
    "print(anomalies['smf'].head())  \n",
    "print(anomalies['upf'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c7d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a description column to each dataset\n",
    "def add_description_column(datasets):\n",
    "    \"\"\"\n",
    "    Add a description column to each dataset for embedding purposes.\n",
    "\n",
    "    Args:\n",
    "        datasets (dict): A dictionary of dataset names and DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated dictionary with description columns added.\n",
    "    \"\"\"\n",
    "    for name, df in datasets.items():\n",
    "        df['description'] = df.apply(\n",
    "            lambda row: f\"{row['timestamp']} - \" +\n",
    "                        \" | \".join([f\"{col}: {row[col]}\" for col in df.columns if col != 'timestamp']),\n",
    "            axis=1\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "# Add descriptions to datasets\n",
    "metrics_data = add_description_column(metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20cc6c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     timestamp  http_connectivity  cpu_utilization_smf_amf  \\\n",
      "181 2025-01-13 13:24:31.133476               True                    False   \n",
      "182 2025-01-13 13:25:31.133476               True                    False   \n",
      "183 2025-01-13 13:26:31.133476               True                    False   \n",
      "\n",
      "     memory_utilization_smf_amf  registration_rate  session_setup_rate  \\\n",
      "181                       False              False               False   \n",
      "182                       False              False               False   \n",
      "183                       False              False               False   \n",
      "\n",
      "     authentication_success_rate  n1n2_message_rate  \\\n",
      "181                        False              False   \n",
      "182                        False              False   \n",
      "183                        False              False   \n",
      "\n",
      "     registration_success_rate  slice_selection_success_rate  ...  \\\n",
      "181                      False                         False  ...   \n",
      "182                      False                         False  ...   \n",
      "183                      False                         False  ...   \n",
      "\n",
      "     cpu_utilization  memory_utilization  packet_processing_rate  \\\n",
      "181            False               False                   False   \n",
      "182            False               False                   False   \n",
      "183            False               False                   False   \n",
      "\n",
      "     tunnel_establishment_rate  buffer_utilization  qos_flow_success_rate  \\\n",
      "181                      False               False                  False   \n",
      "182                      False               False                  False   \n",
      "183                      False               False                  False   \n",
      "\n",
      "     throughput_mbps  packet_drop_rate  latency_ms  combined_anomaly  \n",
      "181            False             False       False              True  \n",
      "182            False             False       False              True  \n",
      "183            False             False       False              True  \n",
      "\n",
      "[3 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge anomalies for correlation analysis\n",
    "def correlate_anomalies(anomalies):\n",
    "    combined = anomalies['amf']\n",
    "    for name, df in anomalies.items():\n",
    "        if name != 'amf':\n",
    "            combined = pd.merge(combined, df, on='timestamp', suffixes=(f'_{name}_amf', f'_{name}'))\n",
    "    combined['combined_anomaly'] = combined.drop(columns=['timestamp']).any(axis=1)\n",
    "    return combined[combined['combined_anomaly']]\n",
    "\n",
    "correlated_anomalies = correlate_anomalies(anomalies)\n",
    "print(correlated_anomalies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa11c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process metrics for RAG\n",
    "def process_metrics(datasets, column_to_embed):\n",
    "    \"\"\"\n",
    "    Process multiple metrics datasets to create individual vector stores for RAG.\n",
    "\n",
    "    Args:\n",
    "        datasets (dict): A dictionary of dataset names and DataFrames.\n",
    "        column_to_embed (str): The column containing text or descriptions to embed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of vector stores for each dataset.\n",
    "    \"\"\"\n",
    "    vector_stores = {}\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        if column_to_embed not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_to_embed}' not found in the {name} DataFrame.\")\n",
    "\n",
    "        # Convert the column to a list of Document objects\n",
    "        documents = [Document(page_content=row[column_to_embed]) for _, row in df.iterrows()]\n",
    "\n",
    "        # Split the documents into chunks (if applicable)\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Embed the chunks\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        vector_stores[name] = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    return vector_stores\n",
    "\n",
    "# Example usage\n",
    "# Assume `metrics_data` is a dictionary containing the AMF, SMF, and UPF DataFrames.\n",
    "# Each DataFrame should have a column 'description' with text to embed.\n",
    "vector_stores = process_metrics(metrics_data, column_to_embed='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05daebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alerts from JSON\n",
    "def load_alerts(file_path):\n",
    "    \"\"\"\n",
    "    Load alerts from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed alert data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load alerts from the `data/` folder\n",
    "alerts = load_alerts('data/alerts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc4602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided anomalies and system logs, the root cause of the anomalies seems to be related to the 'cpu_utilization_smf_amf' metric. This metric is showing as 'False' during the timestamps where anomalies are detected. \n",
      "\n",
      "The 'cpu_utilization_smf_amf' metric is likely indicating the CPU utilization of the Session Management Function (SMF) and Access and Mobility Management Function (AMF) components of the system. A 'False' value for this metric could indicate that the CPU utilization is exceeding a certain threshold or that there is an issue with the CPU utilization monitoring itself.\n",
      "\n",
      "Looking at the relevant AMF and SMF metrics information, we can see that the CPU utilization values are fluctuating. For example, the CPU utilization for AMF at '2025-01-13 10:57:31.133476' is 37.088859726476, but at '2025-01-13 10:30:31.133476' it is 39.3344585640864. Similarly, for SMF, the CPU utilization at '2025-01-14 01:22:31.133476' is 35.751351937668645, but at '2025-01-14 03:06:31.133476' it is 23.301320436584884. \n",
      "\n",
      "These fluctuations in CPU utilization could be causing the anomalies detected in the system. The system might be experiencing periods of high CPU utilization, causing performance issues and triggering the anomaly detection.\n",
      "\n",
      "To confirm this root cause, further investigation would be needed. This could include checking system logs for any error messages related to CPU utilization, examining the system's CPU usage patterns, and checking if there are any processes or applications that are using a high amount of CPU.\n"
     ]
    }
   ],
   "source": [
    "# Analyze anomalies using LLM\n",
    "def analyze_root_cause(llm, vector_stores, anomalies):\n",
    "    \"\"\"\n",
    "    Analyze the root cause of anomalies using LLM and vector stores.\n",
    "\n",
    "    Args:\n",
    "        llm (ChatOpenAI): The LLM object for analysis.\n",
    "        vector_stores (dict): A dictionary of vector stores for AMF, SMF, and UPF.\n",
    "        anomalies (pd.DataFrame): The DataFrame containing correlated anomalies.\n",
    "\n",
    "    Returns:\n",
    "        str: The root cause analysis generated by the LLM.\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Analyze the following anomalies in the metrics and provide a root cause analysis based on the system logs:\n",
    "\n",
    "    Anomalies:\n",
    "    {anomalies}\n",
    "\n",
    "    Relevant AMF metrics information:\n",
    "    {amf_metrics}\n",
    "\n",
    "    Relevant SMF metrics information:\n",
    "    {smf_metrics}\n",
    "\n",
    "    Relevant UPF metrics information:\n",
    "    {upf_metrics}\n",
    "\n",
    "    Provide a detailed root cause analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"anomalies\", \"amf_metrics\", \"smf_metrics\", \"upf_metrics\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    # Function to fetch relevant metrics information from vector stores\n",
    "    def fetch_relevant_metrics(vector_store, anomalies):\n",
    "        results = []\n",
    "        for anomaly in anomalies.to_dict(orient=\"records\"):\n",
    "            results.extend(vector_store.similarity_search(str(anomaly), k=2))\n",
    "        return \"\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    # Fetch relevant metrics information\n",
    "    amf_metrics = fetch_relevant_metrics(vector_stores['amf'], anomalies)\n",
    "    smf_metrics = fetch_relevant_metrics(vector_stores['smf'], anomalies)\n",
    "    upf_metrics = fetch_relevant_metrics(vector_stores['upf'], anomalies)\n",
    "\n",
    "    # Combine data into the chain\n",
    "    chain = ({\n",
    "        \"anomalies\": RunnablePassthrough(),\n",
    "        \"amf_metrics\": lambda x: amf_metrics,\n",
    "        \"smf_metrics\": lambda x: smf_metrics,\n",
    "        \"upf_metrics\": lambda x: upf_metrics\n",
    "    } | prompt | llm | StrOutputParser())\n",
    "\n",
    "    return chain.invoke(str(anomalies))\n",
    "\n",
    "# Example usage\n",
    "root_cause_analysis = analyze_root_cause(llm, vector_stores, correlated_anomalies)\n",
    "print(root_cause_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af532d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided system logs, alerts, and metrics, the anomalies seem to be related to the following issues:\n",
      "\n",
      "1. SMF_CPU_OVERLOAD_CASCADE: This alert indicates that the Session Management Function (SMF) and Access and Mobility Management Function (AMF) components are experiencing a CPU overload. This is a critical issue as it can lead to performance degradation and service disruption. The root cause could be due to an increase in the number of sessions or a sudden surge in network traffic. The CPU utilization metrics for both SMF and AMF show high values, confirming the overload.\n",
      "\n",
      "2. UPF_DATAPATH_DEGRADATION: This alert indicates that the User Plane Function (UPF) and SMF components are experiencing datapath degradation. This is a major issue as it can affect the data flow in the network. The root cause could be due to issues in the network infrastructure such as faulty hardware or network congestion. The packet_processing_rate and tunnel_establishment_rate metrics for UPF show high values, indicating potential datapath issues.\n",
      "\n",
      "3. REGISTRATION_STORM: This alert indicates that the AMF and SMF components are affected by a registration storm. This is a critical issue as it can overload the network with unnecessary registration requests. The root cause could be due to a large number of devices trying to register to the network at the same time. The registration_rate metric for AMF shows high values, confirming the registration storm.\n",
      "\n",
      "In conclusion, the anomalies are likely caused by a combination of CPU overload, datapath degradation, and a registration storm. These issues can lead to network performance degradation and service disruption. Further investigation is needed to identify the exact causes and implement appropriate solutions.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RCA with alerts\n",
    "def analyze_with_alerts(llm, vector_stores, anomalies, alerts):\n",
    "    \"\"\"\n",
    "    Enhance RCA with alerts information.\n",
    "\n",
    "    Args:\n",
    "        llm (ChatOpenAI): The LLM for analysis.\n",
    "        vector_stores (dict): Vector stores for AMF, SMF, and UPF metrics.\n",
    "        anomalies (pd.DataFrame): DataFrame containing anomalies.\n",
    "        alerts (dict): Parsed alert data from alerts.json.\n",
    "\n",
    "    Returns:\n",
    "        str: Detailed RCA combining anomalies and alert information.\n",
    "    \"\"\"\n",
    "    # Parse alerts\n",
    "    alert_details = \"\\n\".join([\n",
    "        f\"Type: {alert['type']}, Severity: {alert['severity']}, Component: {alert['component']}, \"\n",
    "        f\"Start: {alert['start_time']}, End: {alert['end_time']}, Description: {alert['description']}\"\n",
    "        for alert in alerts['alerts']\n",
    "    ])\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt_template = \"\"\"\n",
    "    Analyze the following anomalies in the metrics and provide a root cause analysis based on the system logs and alerts:\n",
    "\n",
    "    Anomalies:\n",
    "    {anomalies}\n",
    "\n",
    "    Alerts:\n",
    "    {alerts}\n",
    "\n",
    "    Relevant AMF metrics information:\n",
    "    {amf_metrics}\n",
    "\n",
    "    Relevant SMF metrics information:\n",
    "    {smf_metrics}\n",
    "\n",
    "    Relevant UPF metrics information:\n",
    "    {upf_metrics}\n",
    "\n",
    "    Provide a detailed root cause analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"anomalies\", \"alerts\", \"amf_metrics\", \"smf_metrics\", \"upf_metrics\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    # Fetch relevant metrics\n",
    "    def fetch_metrics(vector_store, anomalies):\n",
    "        results = []\n",
    "        for anomaly in anomalies.to_dict(orient=\"records\"):\n",
    "            results.extend(vector_store.similarity_search(str(anomaly), k=2))\n",
    "        return \"\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    amf_metrics = fetch_metrics(vector_stores['amf'], anomalies)\n",
    "    smf_metrics = fetch_metrics(vector_stores['smf'], anomalies)\n",
    "    upf_metrics = fetch_metrics(vector_stores['upf'], anomalies)\n",
    "\n",
    "    # Combine into chain\n",
    "    chain = ({\n",
    "        \"anomalies\": RunnablePassthrough(),\n",
    "        \"alerts\": lambda x: alert_details,\n",
    "        \"amf_metrics\": lambda x: amf_metrics,\n",
    "        \"smf_metrics\": lambda x: smf_metrics,\n",
    "        \"upf_metrics\": lambda x: upf_metrics\n",
    "    } | prompt | llm | StrOutputParser())\n",
    "\n",
    "    return chain.invoke(str(anomalies))\n",
    "\n",
    "# Example Usage\n",
    "rca_with_alerts = analyze_with_alerts(llm, vector_stores, correlated_anomalies, alerts)\n",
    "print(rca_with_alerts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
